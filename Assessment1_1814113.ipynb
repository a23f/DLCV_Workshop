{"cells":[{"cell_type":"markdown","metadata":{"id":"EpRNh1-L8zuk"},"source":["## Assessment 1: Deep Learning\n","\n","1) Answer all questions.\n","2) This assessment is open-book. You are allowed to refer to any references including online materials, books, notes, codes, github links, etc.\n","3) Copy this notebook to your google drive (click **FILE** > **save a copy in Drive**)\n","4) Upload the answer notebook to your github. \n","5) Submit the assessment by sharing the link to your answer notebook. "]},{"cell_type":"markdown","metadata":{"id":"kjRauIpz8zun"},"source":["\n","\n","\n","\n","**QUESTION 1** \n","\n","One day while wandering around a clothing store at KL East Mall, you stumbled upon a pretty girl who is choosing a dress for Hari Raya. It turns out that the girl is visually impaired and had a hard time distinguishing between an abaya and a kebaya. To help people with the similar situation, you then decided to develop an AI system to identify the type of clothes using a Convolutional Neural Networks (ConvNet). In order to train the network, you decide to use the Fashion MNIST dataset which is freely available on Pytorch.\n"]},{"cell_type":"markdown","metadata":{"id":"Jzzvkxpn8zuo"},"source":["a) Given the problem, what is the most appropriate loss function to use? Justify your answer. **[5 marks]**"]},{"cell_type":"markdown","metadata":{"id":"O0hERYSq8zuo"},"source":["\n","<span style=\"color:blue\">\n","    ANSWER: YOUR ANSWER HERE (Cross Entropy Loss)</span> "]},{"cell_type":"markdown","metadata":{"id":"CW6A4Pmj8zuo"},"source":["b) Create and train a ConvNet corresponding to the following CNN architecture (with a modification of the final layer to address the number of classes). Please include **[10 marks]**:\n","\n","    1) The dataloader to load the train and test datasets.\n","\n","    2) The model definition (either using sequential method OR pytorch class method).\n","\n","    3) Define your training loop.\n","\n","    4) Output the mean accuracy for the whole testing dataset.\n","\n","    \n","\n","<div>\n","<img src=\"https://vitalflux.com/wp-content/uploads/2021/11/VGG16-CNN-Architecture.png\" width=\"550\"/>\n","</div>\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4g8W6t9DfJK","executionInfo":{"status":"ok","timestamp":1659325063791,"user_tz":-480,"elapsed":20171,"user":{"displayName":"Ahmad Firdaus Ghazali","userId":"01333936129087054527"}},"outputId":"65c584f2-0eb9-4562-db6d-8288af704376"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ue0OHCL8zup","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659328598115,"user_tz":-480,"elapsed":224630,"user":{"displayName":"Ahmad Firdaus Ghazali","userId":"01333936129087054527"}},"outputId":"f12980bc-1f59-441c-ca2d-94aaf358bfb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["60000\n","10000\n","Epoch: 1/10\n","Epoch : 000, Training: Loss: 1.4114, Accuracy: 50.9267%, \n","\t\tValidation : Loss : 0.6491, Accuracy: 74.7300%, Time: 22.3493s\n","Epoch: 2/10\n","Epoch : 001, Training: Loss: 0.5492, Accuracy: 79.4350%, \n","\t\tValidation : Loss : 0.5379, Accuracy: 79.9800%, Time: 22.4672s\n","Epoch: 3/10\n","Epoch : 002, Training: Loss: 0.4573, Accuracy: 83.1250%, \n","\t\tValidation : Loss : 0.4737, Accuracy: 82.6900%, Time: 22.1978s\n","Epoch: 4/10\n","Epoch : 003, Training: Loss: 0.4077, Accuracy: 85.0000%, \n","\t\tValidation : Loss : 0.4282, Accuracy: 84.4700%, Time: 23.4096s\n","Epoch: 5/10\n","Epoch : 004, Training: Loss: 0.3716, Accuracy: 86.2567%, \n","\t\tValidation : Loss : 0.3948, Accuracy: 85.3300%, Time: 21.8772s\n","Epoch: 6/10\n","Epoch : 005, Training: Loss: 0.3505, Accuracy: 86.9967%, \n","\t\tValidation : Loss : 0.3722, Accuracy: 86.3200%, Time: 22.0673s\n","Epoch: 7/10\n","Epoch : 006, Training: Loss: 0.3320, Accuracy: 87.8183%, \n","\t\tValidation : Loss : 0.3636, Accuracy: 86.7300%, Time: 21.8310s\n","Epoch: 8/10\n","Epoch : 007, Training: Loss: 0.3151, Accuracy: 88.4050%, \n","\t\tValidation : Loss : 0.3412, Accuracy: 87.5200%, Time: 23.0785s\n","Epoch: 9/10\n","Epoch : 008, Training: Loss: 0.3011, Accuracy: 88.8033%, \n","\t\tValidation : Loss : 0.3504, Accuracy: 87.0900%, Time: 21.8629s\n","Epoch: 10/10\n","Epoch : 009, Training: Loss: 0.2921, Accuracy: 89.1967%, \n","\t\tValidation : Loss : 0.3324, Accuracy: 87.9600%, Time: 22.7170s\n"]}],"source":["import torch, torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import glob\n","import numpy\n","import random\n","import pandas as pd\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, models, transforms\n","from torchsummary import summary\n","from torchvision.transforms import ToTensor\n","\n","###############################################\n","######## THE REST OF YOUR CODES HERE ##########\n","###############################################\n","\n","# Applying Transforms to the Data\n","image_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize(32),        # Resize the image to 32x32 (supposedly resized to 224 but changed to 32 to reduce training time)\n","        #transforms.Grayscale(num_output_channels=3),    # Convert image to return 3 channel output image\n","        #transforms.RandomResizedCrop(size=32, scale=(0.8, 1.0)),\n","        #transforms.RandomRotation(degrees=15),\n","        #transforms.RandomHorizontalFlip(),\n","        #transforms.CenterCrop(size=32),\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n","        transforms.Normalize([0.5,0.5,0.5],\n","                             [0.5,0.5,0.5])      # 3 Channel Gray image normalization\n","    ]),\n","\n","    'test': transforms.Compose([\n","        transforms.Resize(size=32),   # Resize the image to 32x32 (supposedly resized to 224 but changed to 32 to reduce training time)\n","        #transforms.Grayscale(num_output_channels=3),    # Convert image to return 3 channel output image\n","        #transforms.CenterCrop(size=32),\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n","        transforms.Normalize([0.5,0.5,0.5],\n","                             [0.5,0.5,0.5])       # 3 Channel Gray image normalization\n","    ])\n","}\n","\n","# Loading the Data\n","batch_size = 32\n","\n","trainset = torchvision.datasets.FashionMNIST(root='/content/drive/MyDrive/Assesment1/Fashion_MNIST/', train=True,\n","                                        download=True, transform=image_transforms['train'])\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.FashionMNIST(root='/content/drive/MyDrive/Assesment1/Fashion_MNIST/', train=False,\n","                                       download=True, transform=image_transforms['test'])\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('T-Shirt', 'Trouser', 'Pullover', 'Dress',\n","       'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n","\n","train_data_size = len(trainloader.dataset)\n","test_data_size = len(testloader.dataset)\n","\n","print(train_data_size)\n","print(test_data_size)\n","\n","# Creating the Model\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        #self.conv3 = nn.Conv2d(128, 256, 1)\n","        #self.conv4 = nn.Conv2d(256, 512, 1)\n","        #self.conv5 = nn.Conv2d(512, 512, 1)\n","        self.pool = nn.MaxPool2d(2, 2)              # kernel size=1x1, stride=2\n","        self.fc6 = nn.Linear(16 * 5 * 5, 200)     # output image size=1x1, channels=512   (if use 224 size image, output size is 7)\n","        self.fc7 = nn.Linear(200, 100)\n","        self.fc8 = nn.Linear(100, 10)              # output class=10\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.pool(self.relu(self.conv1(x)))\n","        x = self.pool(self.relu(self.conv2(x)))\n","        #x = self.pool(self.relu(self.conv3(x)))\n","        #x = self.pool(self.relu(self.conv4(x)))\n","        #x = self.pool(self.relu(self.conv5(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = self.relu(self.fc6(x))\n","        x = self.relu(self.fc7(x))\n","        x = self.relu(self.fc8(x))\n","        return x\n","\n","model = CNN()\n","\n","# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# Move the model to GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Calculate the training time\n","import time \n","\n","def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n","    '''\n","    Function to train and validate\n","    Parameters\n","        :param model: Model to train and validate\n","        :param loss_criterion: Loss Criterion to minimize\n","        :param optimizer: Optimizer for computing gradients\n","        :param epochs: Number of epochs (default=25)\n","  \n","    Returns\n","        model: Trained Model with best validation accuracy\n","        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n","    '''\n","    \n","    start = time.time()\n","    history = []\n","    best_acc = 0.0\n","\n","    for epoch in range(epochs):\n","        epoch_start = time.time()\n","        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n","        \n","        # Set to training mode\n","        model.train()\n","        \n","        # Loss and Accuracy within the epoch\n","        train_loss = 0.0\n","        train_acc = 0.0\n","        \n","        valid_loss = 0.0\n","        valid_acc = 0.0\n","        \n","        for i, (inputs, labels) in enumerate(trainloader):\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            \n","            # Clean existing gradients\n","            optimizer.zero_grad()\n","            \n","            # Forward pass - compute outputs on input data using the model\n","            outputs = model(inputs)\n","            \n","            # Compute loss\n","            loss = loss_criterion(outputs, labels)\n","            \n","            # Backpropagate the gradients\n","            loss.backward()\n","            \n","            # Update the parameters\n","            optimizer.step()\n","            \n","            # Compute the total loss for the batch and add it to train_loss\n","            train_loss += loss.item() * inputs.size(0)\n","            \n","            # Compute the accuracy\n","            ret, predictions = torch.max(outputs.data, 1)\n","            correct_counts = predictions.eq(labels.data.view_as(predictions))\n","            \n","            # Convert correct_counts to float and then compute the mean\n","            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","            \n","            # Compute total accuracy in the whole batch and add to train_acc\n","            train_acc += acc.item() * inputs.size(0)\n","            \n","            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n","\n","            \n","        # Validation - No gradient tracking needed\n","        with torch.no_grad():\n","\n","            # Set to evaluation mode\n","            model.eval()\n","\n","            # Validation loop\n","            for j, (inputs, labels) in enumerate(testloader):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # Forward pass - compute outputs on input data using the model\n","                outputs = model(inputs)\n","\n","                # Compute loss\n","                loss = loss_criterion(outputs, labels)\n","\n","                # Compute the total loss for the batch and add it to valid_loss\n","                valid_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate validation accuracy\n","                ret, predictions = torch.max(outputs.data, 1)\n","                correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","                # Convert correct_counts to float and then compute the mean\n","                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","                # Compute total accuracy in the whole batch and add to valid_acc\n","                valid_acc += acc.item() * inputs.size(0)\n","\n","                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n","            \n","        # Find average training loss and training accuracy\n","        avg_train_loss = train_loss/train_data_size \n","        avg_train_acc = train_acc/train_data_size\n","\n","        # Find average training loss and training accuracy\n","        avg_test_loss = valid_loss/test_data_size \n","        avg_test_acc = valid_acc/test_data_size\n","\n","        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n","                \n","        epoch_end = time.time()\n","    \n","        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n","            \n","    return model, history\n","\n","# Train the model for 10 epochs\n","num_epochs = 10\n","trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"]},{"cell_type":"markdown","metadata":{"id":"z1Zjsjxq8zuq"},"source":["c) Replace your defined CNN in b) with a pre-trained model. Then, proceed with a transfer learning and finetune the model for the Fashion MNIST dataset. **[10 marks]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4joDd5u8zur","colab":{"base_uri":"https://localhost:8080/","height":677,"referenced_widgets":["408ab0dbe4e74807b894ff5cb9d2573b","da542b18762b46719fa29e0fcc99d8b2","8d080be0b97e42ebae951db30d73c565","f205e8afb32e485f864c474bce5ab8e5","0a5445134fcd4f91b09fdd91f2c20d5b","6309bed8edf742fca7594e8aa47d2968","ca46a4dba9fb45d48fd9d541ecd05bee","dd79f8a347884c8d9067bb6c604bba63","7616ab1dca3449418511d552c539937a","4a0a3fd19df9465eab9cb2af76a0cc4d","54131bd4f6cc46198b4133d0b8030427"]},"executionInfo":{"status":"ok","timestamp":1659330955783,"user_tz":-480,"elapsed":1706281,"user":{"displayName":"Ahmad Firdaus Ghazali","userId":"01333936129087054527"}},"outputId":"6bef3b49-2af1-443c-f363-5c86161c765d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/230M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"408ab0dbe4e74807b894ff5cb9d2573b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 1/10\n","Epoch : 000, Training: Loss: 0.4469, Accuracy: 85.9033%, \n","\t\tValidation : Loss : 0.2600, Accuracy: 90.3500%, Time: 171.3074s\n","Epoch: 2/10\n","Epoch : 001, Training: Loss: 0.2348, Accuracy: 91.5933%, \n","\t\tValidation : Loss : 0.2638, Accuracy: 90.8800%, Time: 169.9983s\n","Epoch: 3/10\n","Epoch : 002, Training: Loss: 0.1902, Accuracy: 93.0683%, \n","\t\tValidation : Loss : 0.2478, Accuracy: 91.0400%, Time: 170.6333s\n","Epoch: 4/10\n","Epoch : 003, Training: Loss: 0.1685, Accuracy: 93.8050%, \n","\t\tValidation : Loss : 0.2322, Accuracy: 91.9700%, Time: 170.7317s\n","Epoch: 5/10\n","Epoch : 004, Training: Loss: 0.1381, Accuracy: 94.8033%, \n","\t\tValidation : Loss : 0.2222, Accuracy: 92.2800%, Time: 170.6529s\n","Epoch: 6/10\n","Epoch : 005, Training: Loss: 0.1149, Accuracy: 95.8083%, \n","\t\tValidation : Loss : 0.2275, Accuracy: 92.5800%, Time: 170.0805s\n","Epoch: 7/10\n","Epoch : 006, Training: Loss: 0.0959, Accuracy: 96.4600%, \n","\t\tValidation : Loss : 0.2506, Accuracy: 92.2100%, Time: 170.2837s\n","Epoch: 8/10\n","Epoch : 007, Training: Loss: 0.0842, Accuracy: 96.8967%, \n","\t\tValidation : Loss : 0.2610, Accuracy: 92.3900%, Time: 170.4373s\n","Epoch: 9/10\n","Epoch : 008, Training: Loss: 0.0717, Accuracy: 97.4100%, \n","\t\tValidation : Loss : 0.3076, Accuracy: 92.0500%, Time: 170.2850s\n","Epoch: 10/10\n","Epoch : 009, Training: Loss: 0.0620, Accuracy: 97.6800%, \n","\t\tValidation : Loss : 0.2840, Accuracy: 92.1300%, Time: 168.8554s\n"]}],"source":["###############################################\n","###############YOUR CODES HERE ################\n","###############################################\n","\n","model_pretrained = torchvision.models.resnet152(pretrained=True)\n","# 2. LOSS AND OPTIMIZER\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model_pretrained.parameters(), lr=0.001, momentum=0.9)\n","\n","# 3. move the model to GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model_pretrained.to(device)\n","\n","# Train the model for 10 epochs\n","num_epochs = 10\n","trained_model, history = train_and_validate(model_pretrained, criterion, optimizer, num_epochs)"]},{"cell_type":"markdown","metadata":{"id":"o6uCpzFC8zur"},"source":["d) Using model-centric methods, propose two (2) strategies that can be used to increase the accuracy of the model on the testing dataset. **[5 marks]**\n","\n","\n","<span style=\"color:blue\">\n","    Two model-centric techniques that I propose are: \n","    1. Fine Tune the lower layer of the pre-trained model\n","    2. PyTorch batch normalization </span>"]},{"cell_type":"markdown","metadata":{"id":"4FIMfUfz8zur"},"source":["e) Next, implement the two proposed model-centric techniques for the same problem as in the previous question. **[15 marks]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UIGCk5K8zus","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659333723200,"user_tz":-480,"elapsed":771782,"user":{"displayName":"Ahmad Firdaus Ghazali","userId":"01333936129087054527"}},"outputId":"3731be1e-7821-48ba-f97e-ca75e1434849"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["2048\n","Epoch: 1/10\n","Epoch : 000, Training: Loss: 1.8915, Accuracy: 34.6867%, \n","\t\tValidation : Loss : 1.8451, Accuracy: 61.4000%, Time: 75.8402s\n","Epoch: 2/10\n","Epoch : 001, Training: Loss: 0.8950, Accuracy: 67.5733%, \n","\t\tValidation : Loss : 2.2217, Accuracy: 72.1700%, Time: 77.8057s\n","Epoch: 3/10\n","Epoch : 002, Training: Loss: 0.7710, Accuracy: 72.5100%, \n","\t\tValidation : Loss : 1.6655, Accuracy: 74.7900%, Time: 77.1594s\n","Epoch: 4/10\n","Epoch : 003, Training: Loss: 0.7076, Accuracy: 74.5567%, \n","\t\tValidation : Loss : 1.2607, Accuracy: 76.4400%, Time: 76.4139s\n","Epoch: 5/10\n","Epoch : 004, Training: Loss: 0.6668, Accuracy: 75.7367%, \n","\t\tValidation : Loss : 2.1212, Accuracy: 75.4600%, Time: 79.0639s\n","Epoch: 6/10\n","Epoch : 005, Training: Loss: 0.6436, Accuracy: 76.4200%, \n","\t\tValidation : Loss : 1.5421, Accuracy: 76.2900%, Time: 77.3594s\n","Epoch: 7/10\n","Epoch : 006, Training: Loss: 0.6195, Accuracy: 77.3633%, \n","\t\tValidation : Loss : 1.3750, Accuracy: 77.9800%, Time: 76.7156s\n","Epoch: 8/10\n","Epoch : 007, Training: Loss: 0.5989, Accuracy: 78.1400%, \n","\t\tValidation : Loss : 1.1327, Accuracy: 77.5400%, Time: 76.1119s\n","Epoch: 9/10\n","Epoch : 008, Training: Loss: 0.5838, Accuracy: 78.7133%, \n","\t\tValidation : Loss : 0.9495, Accuracy: 79.1800%, Time: 76.7962s\n","Epoch: 10/10\n","Epoch : 009, Training: Loss: 0.5658, Accuracy: 79.3400%, \n","\t\tValidation : Loss : 0.6636, Accuracy: 79.8200%, Time: 76.8879s\n"]}],"source":["###############################################\n","###############YOUR CODES HERE ################\n","###############################################\n","\n","# 1. Fine Tune the lower layer of the pre-trained model\n","model_conv = torchvision.models.resnet152(pretrained=True)\n","for param in model_conv.parameters():\n","    param.requires_grad = False\n","\n","#Parameters of newly constructed modules have requires_grad=True by default\n","num_ftrs = model_conv.fc.in_features\n","print(num_ftrs)\n","model_conv.fc = nn.Sequential(nn.Linear(num_ftrs, (num_ftrs//2)),nn.ReLU(),\n","                            nn.Linear((num_ftrs//2), (num_ftrs//4)), nn.ReLU(),\n","                            nn.Linear((num_ftrs//4), (num_ftrs//8)), nn.ReLU(),\n","                            nn.Linear((num_ftrs//8), (num_ftrs//16)), nn.ReLU(),\n","                            nn.Linear((num_ftrs//16),10)\n","                            )\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model_conv.to(device)\n","\n","# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model_conv.parameters(), lr=0.001, momentum=0.9)\n","\n","# Train the model for 10 epochs\n","num_epochs = 10\n","trained_model, history = train_and_validate(model_conv, criterion, optimizer, num_epochs)"]},{"cell_type":"code","source":["# 2. PyTorch batch normalization\n","\n","class CNN_batch(nn.Module):\n","    def __init__(self):\n","        super(CNN_batch, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.pool = nn.MaxPool2d(2, 2)              # kernel size=1x1, stride=2\n","        self.batchnorm1 = nn.BatchNorm2d(6)         # adding batchnorm to the model\n","        self.batchnorm2 = nn.BatchNorm2d(16)\n","        self.fc6 = nn.Linear(16 * 5 * 5, 200)     # output image size=1x1, channels=512   (if use 224 size image, output size is 7)\n","        self.fc7 = nn.Linear(200, 100)\n","        self.fc8 = nn.Linear(100, 10)              # output class=10\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.pool(self.relu(self.conv1(x)))\n","        x = self.batchnorm1(x)\n","        x = self.pool(self.relu(self.conv2(x)))\n","        x = self.batchnorm2(x)\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = self.relu(self.fc6(x))\n","        x = self.relu(self.fc7(x))\n","        x = self.relu(self.fc8(x))\n","        return x\n","\n","model_batchnorm = CNN_batch()\n","\n","# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model_batchnorm.parameters(), lr=0.001, momentum=0.9)\n","\n","# Move the model to GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model_batchnorm.to(device)\n","\n","# Train the model for 10 epochs\n","num_epochs = 10\n","trained_model, history = train_and_validate(model_conv, criterion, optimizer, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mduKAYUV-BEN","executionInfo":{"status":"ok","timestamp":1659335068494,"user_tz":-480,"elapsed":759247,"user":{"displayName":"Ahmad Firdaus Ghazali","userId":"01333936129087054527"}},"outputId":"4f04aa79-eee3-42c6-8cc0-aeb7166f0bbb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/10\n","Epoch : 000, Training: Loss: 0.5443, Accuracy: 79.9783%, \n","\t\tValidation : Loss : 0.8624, Accuracy: 79.6000%, Time: 78.0406s\n","Epoch: 2/10\n","Epoch : 001, Training: Loss: 0.5445, Accuracy: 80.0867%, \n","\t\tValidation : Loss : 0.9642, Accuracy: 79.5000%, Time: 76.5401s\n","Epoch: 3/10\n","Epoch : 002, Training: Loss: 0.5425, Accuracy: 80.2667%, \n","\t\tValidation : Loss : 0.6397, Accuracy: 80.6300%, Time: 77.9094s\n","Epoch: 4/10\n","Epoch : 003, Training: Loss: 0.5416, Accuracy: 80.1233%, \n","\t\tValidation : Loss : 1.0285, Accuracy: 78.8500%, Time: 76.6242s\n","Epoch: 5/10\n","Epoch : 004, Training: Loss: 0.5411, Accuracy: 80.2417%, \n","\t\tValidation : Loss : 0.7390, Accuracy: 79.2800%, Time: 74.6013s\n","Epoch: 6/10\n","Epoch : 005, Training: Loss: 0.5472, Accuracy: 79.8433%, \n","\t\tValidation : Loss : 0.9099, Accuracy: 79.9800%, Time: 75.2368s\n","Epoch: 7/10\n","Epoch : 006, Training: Loss: 0.5398, Accuracy: 80.1100%, \n","\t\tValidation : Loss : 1.0097, Accuracy: 79.4000%, Time: 75.6008s\n","Epoch: 8/10\n","Epoch : 007, Training: Loss: 0.5400, Accuracy: 80.0733%, \n","\t\tValidation : Loss : 0.7369, Accuracy: 79.5600%, Time: 74.4895s\n","Epoch: 9/10\n","Epoch : 008, Training: Loss: 0.5442, Accuracy: 79.9200%, \n","\t\tValidation : Loss : 0.7666, Accuracy: 79.5300%, Time: 74.9843s\n","Epoch: 10/10\n","Epoch : 009, Training: Loss: 0.5450, Accuracy: 79.9550%, \n","\t\tValidation : Loss : 1.0019, Accuracy: 79.3600%, Time: 74.8455s\n"]}]},{"cell_type":"markdown","metadata":{"id":"DzPPxsCX8zus"},"source":["f) Do you see any accuracy improvement? Whether it is a \"yes\" or \"no\", discuss the possible reasons contributing to the accuracy improvement/ unimprovement. **[5 marks]**\n","\n","<span style=\"color:blue\">\n","    Your answer here: NO. Batch normalization may cause inaccurate estimation of batch statistics when we have a small batch size. This increases the model error. In tasks such as image segmentation, the batch size is usually too small. BN needs a sufficiently large batch size. </span>"]},{"cell_type":"markdown","metadata":{"id":"rVArqW8h8zus"},"source":["g) In real applications, data-centric strategies are essential to train robust deep learning models. Give two (2) examples of such strategies and discuss how the strategies helps improving the model accuracy. **[5 marks]**\n","\n","<span style=\"color:blue\">\n","    Your answer here </span>"]},{"cell_type":"markdown","metadata":{"id":"-zifLt-s8zut"},"source":["h) Next, implement the two proposed data-centric techniques for the same problem as in the previous question. **[10 marks]**\n","1. Data Augmentation - Random resized crop & Center Crop\n","2. 2. Data Augmentation - Random rotation & Random horizontal flip"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"rHNqMSvg8zut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659343302012,"user_tz":-480,"elapsed":351,"user":{"displayName":"Hajar 010","userId":"00191370975495187311"}},"outputId":"3364c005-2250-43ed-bf32-ce2cdca4d296"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CNN(\n","  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc6): Linear(in_features=400, out_features=200, bias=True)\n","  (fc7): Linear(in_features=200, out_features=100, bias=True)\n","  (fc8): Linear(in_features=100, out_features=10, bias=True)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":10}],"source":["###############################################\n","##############YOUR CODES HERE #################\n","###############################################\n","import torch, torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import glob\n","import numpy\n","import random\n","import pandas as pd\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, models, transforms\n","from torchsummary import summary\n","from torchvision.transforms import ToTensor\n","\n","# 1. Data Augmentation - Random resized crop & Center Crop\n","\n","image_transforms_2 = {\n","    'train': transforms.Compose([\n","        transforms.Resize(32),        # Resize the image to 32x32 (supposedly resized to 224 but changed to 32 to reduce training time)\n","        transforms.RandomResizedCrop(size=32, scale=(0.8, 1.0)),\n","        transforms.CenterCrop(size=32),\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n","        transforms.Normalize([0.5,0.5,0.5],\n","                             [0.5,0.5,0.5])      # 3 Channel Gray image normalization\n","    ]),\n","\n","    'test': transforms.Compose([\n","        transforms.Resize(size=32),   # Resize the image to 32x32 (supposedly resized to 224 but changed to 32 to reduce training time)\n","        transforms.CenterCrop(size=32),\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n","        transforms.Normalize([0.5,0.5,0.5],\n","                             [0.5,0.5,0.5])       # 3 Channel Gray image normalization\n","    ])\n","}\n","\n","batch_size = 32\n","\n","classes = ('T-Shirt', 'Trouser', 'Pullover', 'Dress',\n","       'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n","\n","trainset2 = torchvision.datasets.FashionMNIST(root='/content/drive/MyDrive/Assesment1/Fashion_MNIST/', train=True,\n","                                        download=True, transform=image_transforms_2['train'])\n","trainloader2 = torch.utils.data.DataLoader(trainset2, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset2 = torchvision.datasets.FashionMNIST(root='/content/drive/MyDrive/Assesment1/Fashion_MNIST/', train=False,\n","                                       download=True, transform=image_transforms_2['test'])\n","testloader2 = torch.utils.data.DataLoader(testset2, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","train_data_size2 = len(trainloader2.dataset)\n","test_data_size2 = len(testloader2.dataset)\n","\n","# 2. Data Augmentation - Random rotation & Random horizontal flip\n","\n","image_transforms_3 = {\n","    'train': transforms.Compose([\n","        transforms.Resize(32),        # Resize the image to 32x32 (supposedly resized to 224 but changed to 32 to reduce training time)\n","        transforms.RandomRotation(degrees=15),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n","        transforms.Normalize([0.5,0.5,0.5],\n","                             [0.5,0.5,0.5])      # 3 Channel Gray image normalization\n","    ]),\n","\n","    'test': transforms.Compose([\n","        transforms.Resize(size=32),   # Resize the image to 32x32 (supposedly resized to 224 but changed to 32 to reduce training time)\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n","        transforms.Normalize([0.5,0.5,0.5],\n","                             [0.5,0.5,0.5])       # 3 Channel Gray image normalization\n","    ])\n","}\n","\n","trainset3 = torchvision.datasets.FashionMNIST(root='/content/drive/MyDrive/Assesment1/Fashion_MNIST/', train=True,\n","                                        download=True, transform=image_transforms_3['train'])\n","trainloader3 = torch.utils.data.DataLoader(trainset2, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset3 = torchvision.datasets.FashionMNIST(root='/content/drive/MyDrive/Assesment1/Fashion_MNIST/', train=False,\n","                                       download=True, transform=image_transforms_3['test'])\n","testloader3 = torch.utils.data.DataLoader(testset2, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","train_data_size3 = len(trainloader3.dataset)\n","test_data_size3 = len(testloader3.dataset)\n","\n","# Define the Model for both Augmentation\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        #self.conv3 = nn.Conv2d(128, 256, 1)\n","        #self.conv4 = nn.Conv2d(256, 512, 1)\n","        #self.conv5 = nn.Conv2d(512, 512, 1)\n","        self.pool = nn.MaxPool2d(2, 2)              # kernel size=1x1, stride=2\n","        self.fc6 = nn.Linear(16 * 5 * 5, 200)     # output image size=1x1, channels=512   (if use 224 size image, output size is 7)\n","        self.fc7 = nn.Linear(200, 100)\n","        self.fc8 = nn.Linear(100, 10)              # output class=10\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.pool(self.relu(self.conv1(x)))\n","        x = self.pool(self.relu(self.conv2(x)))\n","        #x = self.pool(self.relu(self.conv3(x)))\n","        #x = self.pool(self.relu(self.conv4(x)))\n","        #x = self.pool(self.relu(self.conv5(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = self.relu(self.fc6(x))\n","        x = self.relu(self.fc7(x))\n","        x = self.relu(self.fc8(x))\n","        return x\n","\n","model = CNN()\n","\n","# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# Move the model to GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"]},{"cell_type":"code","source":["# Training for 1. Data Augmentation - Random resized crop & Center Crop\n","import time \n","\n","def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n","    '''\n","    Function to train and validate\n","    Parameters\n","        :param model: Model to train and validate\n","        :param loss_criterion: Loss Criterion to minimize\n","        :param optimizer: Optimizer for computing gradients\n","        :param epochs: Number of epochs (default=25)\n","  \n","    Returns\n","        model: Trained Model with best validation accuracy\n","        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n","    '''\n","    \n","    start = time.time()\n","    history2 = []\n","    best_acc = 0.0\n","\n","    for epoch in range(epochs):\n","        epoch_start = time.time()\n","        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n","        \n","        # Set to training mode\n","        model.train()\n","        \n","        # Loss and Accuracy within the epoch\n","        train_loss = 0.0\n","        train_acc = 0.0\n","        \n","        valid_loss = 0.0\n","        valid_acc = 0.0\n","        \n","        for i, (inputs, labels) in enumerate(trainloader2):\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            \n","            # Clean existing gradients\n","            optimizer.zero_grad()\n","            \n","            # Forward pass - compute outputs on input data using the model\n","            outputs = model(inputs)\n","            \n","            # Compute loss\n","            loss = loss_criterion(outputs, labels)\n","            \n","            # Backpropagate the gradients\n","            loss.backward()\n","            \n","            # Update the parameters\n","            optimizer.step()\n","            \n","            # Compute the total loss for the batch and add it to train_loss\n","            train_loss += loss.item() * inputs.size(0)\n","            \n","            # Compute the accuracy\n","            ret, predictions = torch.max(outputs.data, 1)\n","            correct_counts = predictions.eq(labels.data.view_as(predictions))\n","            \n","            # Convert correct_counts to float and then compute the mean\n","            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","            \n","            # Compute total accuracy in the whole batch and add to train_acc\n","            train_acc += acc.item() * inputs.size(0)\n","            \n","            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n","\n","            \n","        # Validation - No gradient tracking needed\n","        with torch.no_grad():\n","\n","            # Set to evaluation mode\n","            model.eval()\n","\n","            # Validation loop\n","            for j, (inputs, labels) in enumerate(testloader2):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # Forward pass - compute outputs on input data using the model\n","                outputs = model(inputs)\n","\n","                # Compute loss\n","                loss = loss_criterion(outputs, labels)\n","\n","                # Compute the total loss for the batch and add it to valid_loss\n","                valid_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate validation accuracy\n","                ret, predictions = torch.max(outputs.data, 1)\n","                correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","                # Convert correct_counts to float and then compute the mean\n","                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","                # Compute total accuracy in the whole batch and add to valid_acc\n","                valid_acc += acc.item() * inputs.size(0)\n","\n","                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n","            \n","        # Find average training loss and training accuracy\n","        avg_train_loss = train_loss/train_data_size2 \n","        avg_train_acc = train_acc/train_data_size2\n","\n","        # Find average training loss and training accuracy\n","        avg_test_loss = valid_loss/test_data_size2 \n","        avg_test_acc = valid_acc/test_data_size2\n","\n","        history2.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n","                \n","        epoch_end = time.time()\n","    \n","        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n","            \n","    return model, history2\n","\n","# Train the model for 10 epochs\n","num_epochs = 10\n","trained_model, history2 = train_and_validate(model, criterion, optimizer, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y2NE44wLhP-E","executionInfo":{"status":"ok","timestamp":1659343612874,"user_tz":-480,"elapsed":307114,"user":{"displayName":"Hajar 010","userId":"00191370975495187311"}},"outputId":"ce5cafed-d5c1-4145-a1b5-42d957177249"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/10\n","Epoch : 000, Training: Loss: 1.9386, Accuracy: 28.0817%, \n","\t\tValidation : Loss : 1.3386, Accuracy: 49.3900%, Time: 31.3927s\n","Epoch: 2/10\n","Epoch : 001, Training: Loss: 1.2180, Accuracy: 53.5467%, \n","\t\tValidation : Loss : 1.0023, Accuracy: 60.5500%, Time: 30.3998s\n","Epoch: 3/10\n","Epoch : 002, Training: Loss: 0.9983, Accuracy: 60.2783%, \n","\t\tValidation : Loss : 0.9633, Accuracy: 61.9300%, Time: 31.2639s\n","Epoch: 4/10\n","Epoch : 003, Training: Loss: 0.9494, Accuracy: 62.1550%, \n","\t\tValidation : Loss : 0.9170, Accuracy: 62.9800%, Time: 30.4971s\n","Epoch: 5/10\n","Epoch : 004, Training: Loss: 0.9170, Accuracy: 63.3983%, \n","\t\tValidation : Loss : 0.8661, Accuracy: 65.5500%, Time: 30.4044s\n","Epoch: 6/10\n","Epoch : 005, Training: Loss: 0.8910, Accuracy: 64.5200%, \n","\t\tValidation : Loss : 0.8385, Accuracy: 66.5800%, Time: 30.3296s\n","Epoch: 7/10\n","Epoch : 006, Training: Loss: 0.8702, Accuracy: 65.1767%, \n","\t\tValidation : Loss : 0.8215, Accuracy: 67.2400%, Time: 30.9831s\n","Epoch: 8/10\n","Epoch : 007, Training: Loss: 0.8515, Accuracy: 65.8117%, \n","\t\tValidation : Loss : 0.8004, Accuracy: 68.0200%, Time: 30.4383s\n","Epoch: 9/10\n","Epoch : 008, Training: Loss: 0.8378, Accuracy: 66.3533%, \n","\t\tValidation : Loss : 0.8065, Accuracy: 67.8800%, Time: 30.6122s\n","Epoch: 10/10\n","Epoch : 009, Training: Loss: 0.8261, Accuracy: 66.6750%, \n","\t\tValidation : Loss : 0.7957, Accuracy: 68.5400%, Time: 30.3478s\n"]}]},{"cell_type":"code","source":["# Training for 2. Data Augmentation - Random rotation & Random horizontal flip\n","import time \n","\n","def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n","    '''\n","    Function to train and validate\n","    Parameters\n","        :param model: Model to train and validate\n","        :param loss_criterion: Loss Criterion to minimize\n","        :param optimizer: Optimizer for computing gradients\n","        :param epochs: Number of epochs (default=25)\n","  \n","    Returns\n","        model: Trained Model with best validation accuracy\n","        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n","    '''\n","    \n","    start = time.time()\n","    history3 = []\n","    best_acc = 0.0\n","\n","    for epoch in range(epochs):\n","        epoch_start = time.time()\n","        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n","        \n","        # Set to training mode\n","        model.train()\n","        \n","        # Loss and Accuracy within the epoch\n","        train_loss = 0.0\n","        train_acc = 0.0\n","        \n","        valid_loss = 0.0\n","        valid_acc = 0.0\n","        \n","        for i, (inputs, labels) in enumerate(trainloader3):\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            \n","            # Clean existing gradients\n","            optimizer.zero_grad()\n","            \n","            # Forward pass - compute outputs on input data using the model\n","            outputs = model(inputs)\n","            \n","            # Compute loss\n","            loss = loss_criterion(outputs, labels)\n","            \n","            # Backpropagate the gradients\n","            loss.backward()\n","            \n","            # Update the parameters\n","            optimizer.step()\n","            \n","            # Compute the total loss for the batch and add it to train_loss\n","            train_loss += loss.item() * inputs.size(0)\n","            \n","            # Compute the accuracy\n","            ret, predictions = torch.max(outputs.data, 1)\n","            correct_counts = predictions.eq(labels.data.view_as(predictions))\n","            \n","            # Convert correct_counts to float and then compute the mean\n","            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","            \n","            # Compute total accuracy in the whole batch and add to train_acc\n","            train_acc += acc.item() * inputs.size(0)\n","            \n","            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n","\n","            \n","        # Validation - No gradient tracking needed\n","        with torch.no_grad():\n","\n","            # Set to evaluation mode\n","            model.eval()\n","\n","            # Validation loop\n","            for j, (inputs, labels) in enumerate(testloader3):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # Forward pass - compute outputs on input data using the model\n","                outputs = model(inputs)\n","\n","                # Compute loss\n","                loss = loss_criterion(outputs, labels)\n","\n","                # Compute the total loss for the batch and add it to valid_loss\n","                valid_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate validation accuracy\n","                ret, predictions = torch.max(outputs.data, 1)\n","                correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","                # Convert correct_counts to float and then compute the mean\n","                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","                # Compute total accuracy in the whole batch and add to valid_acc\n","                valid_acc += acc.item() * inputs.size(0)\n","\n","                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n","            \n","        # Find average training loss and training accuracy\n","        avg_train_loss = train_loss/train_data_size3 \n","        avg_train_acc = train_acc/train_data_size3\n","\n","        # Find average training loss and training accuracy\n","        avg_test_loss = valid_loss/test_data_size3 \n","        avg_test_acc = valid_acc/test_data_size3\n","\n","        history3.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n","                \n","        epoch_end = time.time()\n","    \n","        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n","            \n","    return model, history3\n","\n","# Train the model for 10 epochs\n","num_epochs = 10\n","trained_model, history3 = train_and_validate(model, criterion, optimizer, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MoLaTWZVhaao","executionInfo":{"status":"ok","timestamp":1659343923284,"user_tz":-480,"elapsed":309147,"user":{"displayName":"Hajar 010","userId":"00191370975495187311"}},"outputId":"30755f0b-4055-465f-ed3c-578964b3f9bc"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/10\n","Epoch : 000, Training: Loss: 0.8120, Accuracy: 67.3750%, \n","\t\tValidation : Loss : 0.8271, Accuracy: 66.7600%, Time: 31.4669s\n","Epoch: 2/10\n","Epoch : 001, Training: Loss: 0.7852, Accuracy: 68.6517%, \n","\t\tValidation : Loss : 0.5781, Accuracy: 78.7200%, Time: 30.4748s\n","Epoch: 3/10\n","Epoch : 002, Training: Loss: 0.5982, Accuracy: 77.4367%, \n","\t\tValidation : Loss : 0.5674, Accuracy: 78.2000%, Time: 30.3609s\n","Epoch: 4/10\n","Epoch : 003, Training: Loss: 0.5864, Accuracy: 77.6150%, \n","\t\tValidation : Loss : 0.5521, Accuracy: 79.1800%, Time: 30.5604s\n","Epoch: 5/10\n","Epoch : 004, Training: Loss: 0.5779, Accuracy: 77.8717%, \n","\t\tValidation : Loss : 0.5691, Accuracy: 78.1700%, Time: 31.6651s\n","Epoch: 6/10\n","Epoch : 005, Training: Loss: 0.5684, Accuracy: 78.1850%, \n","\t\tValidation : Loss : 0.5413, Accuracy: 78.9900%, Time: 30.5012s\n","Epoch: 7/10\n","Epoch : 006, Training: Loss: 0.5619, Accuracy: 78.3450%, \n","\t\tValidation : Loss : 0.5556, Accuracy: 78.3100%, Time: 30.7709s\n","Epoch: 8/10\n","Epoch : 007, Training: Loss: 0.3595, Accuracy: 86.5617%, \n","\t\tValidation : Loss : 0.3128, Accuracy: 88.6900%, Time: 30.6161s\n","Epoch: 9/10\n","Epoch : 008, Training: Loss: 0.3285, Accuracy: 87.6917%, \n","\t\tValidation : Loss : 0.3115, Accuracy: 88.1600%, Time: 31.8381s\n","Epoch: 10/10\n","Epoch : 009, Training: Loss: 0.3238, Accuracy: 87.9433%, \n","\t\tValidation : Loss : 0.3052, Accuracy: 88.7100%, Time: 30.5919s\n"]}]},{"cell_type":"markdown","metadata":{"id":"FCy3b5888zut"},"source":["**QUESTION 2** **[35 marks]**\n","\n","Firstly, watch this video:\n","\n","https://drive.google.com/file/d/1bsypahR7I3f_R3DXkfw_tf0BrbCHxE_O/view?usp=sharing\n","\n","This video shows an example of masked face recognition where the deep learning model is able to detect and classify your face even when wearing a face mask. Using the end-to-end object detection pipeline that you have learned, develop your own masked face recognition such that the model should recognize your face even on face mask while recognize other persons as \"others\".\n","\n","Deliverables for this question are:\n","\n","- the model file. Change the name to <your_name>.pt file (e.g. hasan.pt).\n","- a short video (~10 secs) containing your face and your friends faces (for inference)."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"9oIfLdzS8zut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659343953365,"user_tz":-480,"elapsed":25681,"user":{"displayName":"Hajar 010","userId":"00191370975495187311"}},"outputId":"af808066-ba84-473a-880c-242a978bd0b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 12351, done.\u001b[K\n","remote: Counting objects: 100% (133/133), done.\u001b[K\n","remote: Compressing objects: 100% (87/87), done.\u001b[K\n","remote: Total 12351 (delta 77), reused 89 (delta 46), pack-reused 12218\u001b[K\n","Receiving objects: 100% (12351/12351), 12.40 MiB | 26.66 MiB/s, done.\n","Resolving deltas: 100% (8447/8447), done.\n","/content/yolov5/yolov5\n","Setup complete. Using torch 1.12.0+cu113 (Tesla T4)\n","upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=yolov5&ref=ultralytics\n","loading Roboflow workspace...\n","loading Roboflow project...\n","Downloading Dataset Version Zip in /content/datasets/Person_Recognition-1 to yolov5pytorch: 100% [20978369 / 20978369] bytes\n"]},{"output_type":"stream","name":"stderr","text":["Extracting Dataset Version Zip to /content/datasets/Person_Recognition-1 in yolov5pytorch:: 100%|| 952/952 [00:01<00:00, 908.26it/s] \n"]}],"source":["#clone YOLOv5 and \n","!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","%pip install -qr requirements.txt # install dependencies\n","%pip install -q roboflow\n","\n","import torch\n","import os\n","from IPython.display import Image, clear_output  # to display images\n","\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")\n","\n","from roboflow import Roboflow\n","rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")\n","\n","# set up environment\n","os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\"\n","\n","#after following the link above, recieve python code with these fields filled in\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"5VlKA4iBf9T2rMQ3dx3b\")\n","project = rf.workspace(\"ahmad-firdaus\").project(\"person_recognition\")\n","dataset = project.version(1).download(\"yolov5\")"]},{"cell_type":"code","source":["!python train.py --img 416 --batch 16 --epochs 150 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2XOjyASfJEo","executionInfo":{"status":"ok","timestamp":1659344653424,"user_tz":-480,"elapsed":700064,"user":{"displayName":"Hajar 010","userId":"00191370975495187311"}},"outputId":"1e2292d9-ed75-4865-8e17-0b6a3c7de431"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/datasets/Person_Recognition-1/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=150, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 \n","YOLOv5  v6.1-347-g7b9cc32 Python-3.7.13 torch-1.12.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5  runs (RECOMMENDED)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n","100% 14.1M/14.1M [00:00<00:00, 49.6MB/s]\n","\n","Overriding model.yaml nc=80 with nc=2\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","Model summary: 270 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n","\n","Transferred 343/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n","Scaled weight_decay = 0.0005\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/datasets/Person_Recognition-1/train/labels.cache' images and labels... 411 found, 0 missing, 0 empty, 0 corrupt: 100% 411/411 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 411/411 [00:01<00:00, 237.13it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/datasets/Person_Recognition-1/valid/labels.cache' images and labels... 39 found, 0 missing, 0 empty, 0 corrupt: 100% 39/39 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 39/39 [00:00<00:00, 95.56it/s] \n","Plotting labels to runs/train/exp/labels.jpg... \n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.00 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset \n","Image sizes 416 train, 416 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/train/exp\u001b[0m\n","Starting training for 150 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     0/149     1.71G    0.1032   0.03636   0.02842        69       416: 100% 26/26 [00:09<00:00,  2.62it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.84it/s]\n","                 all         39        111    0.00914      0.965     0.0844     0.0222\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     1/149     2.07G   0.07033   0.04296   0.02147        59       416: 100% 26/26 [00:03<00:00,  6.54it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  5.46it/s]\n","                 all         39        111      0.176      0.758      0.243     0.0883\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     2/149     2.07G   0.07413   0.03239   0.01705        39       416: 100% 26/26 [00:03<00:00,  6.80it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.47it/s]\n","                 all         39        111      0.279      0.749      0.413      0.134\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     3/149     2.07G   0.07133   0.03001   0.01216        64       416: 100% 26/26 [00:03<00:00,  7.08it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.33it/s]\n","                 all         39        111      0.538      0.582      0.562      0.196\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     4/149     2.07G   0.06109   0.02851  0.008474        53       416: 100% 26/26 [00:03<00:00,  7.17it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.67it/s]\n","                 all         39        111      0.584      0.751      0.708      0.266\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     5/149     2.07G   0.05489    0.0258  0.005892        56       416: 100% 26/26 [00:03<00:00,  7.09it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.43it/s]\n","                 all         39        111      0.768      0.797      0.859      0.396\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     6/149     2.07G   0.05022   0.02676  0.004588        49       416: 100% 26/26 [00:03<00:00,  7.17it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.87it/s]\n","                 all         39        111       0.75      0.907      0.846      0.425\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     7/149     2.07G    0.0417   0.02553  0.004492        64       416: 100% 26/26 [00:03<00:00,  6.98it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  5.18it/s]\n","                 all         39        111       0.84      0.893      0.938      0.478\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     8/149     2.07G   0.04254   0.02573  0.003368        66       416: 100% 26/26 [00:04<00:00,  6.19it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.07it/s]\n","                 all         39        111      0.776      0.917      0.897      0.488\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     9/149     2.07G   0.03746   0.02465   0.00296        56       416: 100% 26/26 [00:03<00:00,  7.11it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.23it/s]\n","                 all         39        111      0.947          1      0.971      0.632\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    10/149     2.07G   0.03496   0.02218   0.00234        51       416: 100% 26/26 [00:03<00:00,  7.08it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.00it/s]\n","                 all         39        111      0.935      0.979      0.966      0.666\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    11/149     2.07G    0.0345   0.02382  0.002269        84       416: 100% 26/26 [00:03<00:00,  7.16it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.12it/s]\n","                 all         39        111      0.949          1      0.967      0.669\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    12/149     2.07G   0.03634   0.02294  0.002059        74       416: 100% 26/26 [00:03<00:00,  7.18it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.28it/s]\n","                 all         39        111      0.947          1      0.962      0.607\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    13/149     2.07G   0.03223   0.02274  0.001745        55       416: 100% 26/26 [00:03<00:00,  7.06it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.98it/s]\n","                 all         39        111      0.953          1      0.968      0.745\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    14/149     2.07G   0.03027    0.0209  0.001762        73       416: 100% 26/26 [00:03<00:00,  7.02it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.10it/s]\n","                 all         39        111       0.95          1      0.973      0.744\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    15/149     2.07G   0.03128   0.02147  0.001671        60       416: 100% 26/26 [00:03<00:00,  7.03it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.83it/s]\n","                 all         39        111      0.948          1      0.971      0.751\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    16/149     2.07G   0.03105   0.02079  0.001634        40       416: 100% 26/26 [00:03<00:00,  7.18it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.24it/s]\n","                 all         39        111      0.951      0.993      0.961      0.739\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    17/149     2.07G   0.02989   0.02098  0.001349        57       416: 100% 26/26 [00:03<00:00,  6.74it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.30it/s]\n","                 all         39        111      0.954          1      0.962       0.72\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    18/149     2.07G   0.02954   0.01998   0.00153        33       416: 100% 26/26 [00:03<00:00,  7.25it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.20it/s]\n","                 all         39        111      0.952          1      0.966      0.693\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    19/149     2.07G   0.02929   0.02215  0.001546        74       416: 100% 26/26 [00:03<00:00,  6.77it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  4.59it/s]\n","                 all         39        111      0.943      0.986      0.973      0.703\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    20/149     2.07G    0.0279   0.02104  0.001342        59       416: 100% 26/26 [00:03<00:00,  7.13it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.28it/s]\n","                 all         39        111      0.957      0.986      0.973      0.716\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    21/149     2.07G   0.02714   0.02009  0.001831        49       416: 100% 26/26 [00:03<00:00,  7.13it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.23it/s]\n","                 all         39        111      0.949          1      0.968      0.614\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    22/149     2.07G    0.0286   0.02127  0.001314        59       416: 100% 26/26 [00:03<00:00,  6.97it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.64it/s]\n","                 all         39        111      0.957      0.999       0.97      0.718\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    23/149     2.07G   0.02767   0.02102    0.0012        55       416: 100% 26/26 [00:03<00:00,  7.01it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.08it/s]\n","                 all         39        111      0.958      0.993      0.965      0.748\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    24/149     2.07G   0.02812   0.01903    0.0015        50       416: 100% 26/26 [00:03<00:00,  7.16it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.22it/s]\n","                 all         39        111       0.96          1      0.968      0.689\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    25/149     2.07G   0.02689   0.01931  0.001492        60       416: 100% 26/26 [00:03<00:00,  7.15it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.16it/s]\n","                 all         39        111      0.956      0.997       0.97      0.711\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    26/149     2.07G   0.02638   0.01863 0.0009573        70       416: 100% 26/26 [00:04<00:00,  6.16it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.32it/s]\n","                 all         39        111      0.963          1      0.965      0.656\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    27/149     2.07G   0.02668   0.01899  0.001104        58       416: 100% 26/26 [00:03<00:00,  7.03it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.15it/s]\n","                 all         39        111      0.963          1      0.963      0.708\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    28/149     2.07G   0.02684   0.02093  0.001203        82       416: 100% 26/26 [00:03<00:00,  7.16it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.22it/s]\n","                 all         39        111      0.962          1      0.965      0.762\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    29/149     2.07G   0.02479   0.01904  0.001036        55       416: 100% 26/26 [00:03<00:00,  7.03it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.92it/s]\n","                 all         39        111      0.956          1      0.964      0.764\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    30/149     2.07G   0.02517   0.01888 0.0007457        81       416: 100% 26/26 [00:03<00:00,  7.03it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.04it/s]\n","                 all         39        111      0.956      0.993      0.966      0.747\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    31/149     2.07G   0.02543   0.01945 0.0006793        53       416: 100% 26/26 [00:03<00:00,  7.05it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.06it/s]\n","                 all         39        111      0.963      0.999      0.969      0.767\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    32/149     2.07G   0.02429   0.01817 0.0008981        36       416: 100% 26/26 [00:03<00:00,  7.11it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.84it/s]\n","                 all         39        111      0.963          1      0.964      0.745\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    33/149     2.07G    0.0236   0.01902 0.0007841        70       416: 100% 26/26 [00:03<00:00,  7.07it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.09it/s]\n","                 all         39        111      0.963      0.992      0.962      0.755\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    34/149     2.07G   0.02455    0.0179 0.0009772        49       416: 100% 26/26 [00:03<00:00,  7.14it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.14it/s]\n","                 all         39        111      0.962      0.999      0.969      0.767\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    35/149     2.07G     0.024   0.01859  0.000837        59       416: 100% 26/26 [00:03<00:00,  7.08it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.16it/s]\n","                 all         39        111      0.967      0.993      0.969      0.756\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    36/149     2.07G   0.02467   0.01799 0.0007115        55       416: 100% 26/26 [00:03<00:00,  6.66it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  5.14it/s]\n","                 all         39        111      0.956          1      0.967      0.729\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    37/149     2.07G   0.02333   0.01748 0.0007203        34       416: 100% 26/26 [00:03<00:00,  6.53it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.97it/s]\n","                 all         39        111      0.958          1      0.966      0.769\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    38/149     2.07G   0.02456   0.01766 0.0006954        39       416: 100% 26/26 [00:03<00:00,  7.13it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.25it/s]\n","                 all         39        111       0.95          1      0.965      0.767\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    39/149     2.07G   0.02355    0.0179 0.0007943        62       416: 100% 26/26 [00:03<00:00,  7.21it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.01it/s]\n","                 all         39        111      0.961      0.993      0.966      0.778\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    40/149     2.07G   0.02278   0.01739 0.0006502        51       416: 100% 26/26 [00:03<00:00,  7.15it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.16it/s]\n","                 all         39        111      0.964      0.998      0.964      0.767\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    41/149     2.07G   0.02296   0.01731 0.0007137        29       416: 100% 26/26 [00:03<00:00,  7.07it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.75it/s]\n","                 all         39        111      0.965      0.993      0.975       0.78\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    42/149     2.07G   0.02332   0.01765 0.0008024        43       416: 100% 26/26 [00:03<00:00,  7.05it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.05it/s]\n","                 all         39        111      0.964          1      0.966      0.743\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    43/149     2.07G    0.0241   0.01872 0.0006037        62       416: 100% 26/26 [00:03<00:00,  6.82it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.02it/s]\n","                 all         39        111      0.963          1      0.966      0.779\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    44/149     2.07G   0.02231   0.01802 0.0006535        55       416: 100% 26/26 [00:03<00:00,  6.86it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.84it/s]\n","                 all         39        111      0.963          1      0.963      0.765\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    45/149     2.07G   0.02174   0.01899 0.0005989        82       416: 100% 26/26 [00:03<00:00,  7.00it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.08it/s]\n","                 all         39        111      0.964      0.999      0.963       0.77\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    46/149     2.07G   0.02241   0.01641 0.0006177        37       416: 100% 26/26 [00:03<00:00,  7.09it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.19it/s]\n","                 all         39        111      0.964          1      0.967      0.762\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    47/149     2.07G   0.02241   0.01728 0.0005446        46       416: 100% 26/26 [00:03<00:00,  7.13it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.09it/s]\n","                 all         39        111      0.966          1      0.968      0.769\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    48/149     2.07G   0.02289   0.01733 0.0005273        58       416: 100% 26/26 [00:03<00:00,  7.09it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.41it/s]\n","                 all         39        111      0.955          1      0.967      0.784\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    49/149     2.07G   0.02202   0.01763 0.0005721        54       416: 100% 26/26 [00:03<00:00,  7.13it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.85it/s]\n","                 all         39        111      0.952      0.999      0.967      0.791\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    50/149     2.07G   0.02076   0.01773 0.0005227        83       416: 100% 26/26 [00:03<00:00,  6.99it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.97it/s]\n","                 all         39        111      0.965      0.992      0.966      0.782\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    51/149     2.07G   0.02177   0.01748  0.000538        70       416: 100% 26/26 [00:03<00:00,  7.10it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.92it/s]\n","                 all         39        111      0.958          1      0.968      0.773\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    52/149     2.07G   0.02239   0.01764 0.0005649        55       416: 100% 26/26 [00:03<00:00,  7.23it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.71it/s]\n","                 all         39        111      0.958          1      0.963       0.78\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    53/149     2.07G   0.02153   0.01659 0.0007129        44       416: 100% 26/26 [00:03<00:00,  7.12it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.89it/s]\n","                 all         39        111      0.965      0.997      0.965      0.754\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    54/149     2.07G   0.02207   0.01675  0.000623        55       416: 100% 26/26 [00:03<00:00,  7.06it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.05it/s]\n","                 all         39        111      0.957          1      0.967      0.741\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    55/149     2.07G   0.02149   0.01633 0.0005715        44       416: 100% 26/26 [00:03<00:00,  7.05it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.18it/s]\n","                 all         39        111      0.963          1      0.965      0.756\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    56/149     2.07G   0.02217   0.01742 0.0004902        49       416: 100% 26/26 [00:03<00:00,  7.10it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.17it/s]\n","                 all         39        111      0.964          1      0.969      0.764\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    57/149     2.07G   0.02154   0.01646   0.00038        50       416: 100% 26/26 [00:03<00:00,  7.03it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.28it/s]\n","                 all         39        111      0.963          1      0.968      0.774\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    58/149     2.07G   0.02068   0.01682 0.0003846        49       416: 100% 26/26 [00:03<00:00,  6.91it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.06it/s]\n","                 all         39        111      0.965          1      0.964       0.78\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    59/149     2.07G   0.01987   0.01664 0.0004113        52       416: 100% 26/26 [00:03<00:00,  7.09it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.19it/s]\n","                 all         39        111       0.96          1      0.963      0.771\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    60/149     2.07G   0.02082   0.01622 0.0005813        59       416: 100% 26/26 [00:03<00:00,  7.09it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.24it/s]\n","                 all         39        111      0.961          1      0.964       0.77\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    61/149     2.07G   0.01976    0.0161  0.000555        58       416: 100% 26/26 [00:03<00:00,  7.05it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.18it/s]\n","                 all         39        111      0.964          1      0.963      0.773\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    62/149     2.07G   0.02022   0.01616 0.0008235        35       416: 100% 26/26 [00:03<00:00,  7.09it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.18it/s]\n","                 all         39        111      0.965          1      0.966      0.778\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    63/149     2.07G   0.01963    0.0164 0.0004414        51       416: 100% 26/26 [00:03<00:00,  7.07it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.75it/s]\n","                 all         39        111      0.966          1      0.965      0.775\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    64/149     2.07G   0.02021   0.01686 0.0004095        51       416: 100% 26/26 [00:03<00:00,  6.85it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.08it/s]\n","                 all         39        111      0.966          1      0.963      0.775\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    65/149     2.07G   0.01956   0.01603 0.0004045        42       416: 100% 26/26 [00:03<00:00,  7.10it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.29it/s]\n","                 all         39        111      0.966          1      0.969      0.778\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    66/149     2.07G    0.0191   0.01581 0.0005433        42       416: 100% 26/26 [00:04<00:00,  5.43it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.12it/s]\n","                 all         39        111      0.966          1      0.968      0.776\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    67/149     2.07G   0.01962   0.01576 0.0004368        42       416: 100% 26/26 [00:03<00:00,  7.14it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.96it/s]\n","                 all         39        111      0.965          1      0.969      0.781\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    68/149     2.07G   0.01976   0.01584 0.0004579        43       416: 100% 26/26 [00:03<00:00,  7.05it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.90it/s]\n","                 all         39        111      0.965          1      0.968      0.774\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    69/149     2.07G   0.01924   0.01521 0.0005646        46       416: 100% 26/26 [00:03<00:00,  7.00it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.98it/s]\n","                 all         39        111      0.966          1      0.969      0.795\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    70/149     2.07G   0.02026    0.0159 0.0004032        63       416: 100% 26/26 [00:03<00:00,  7.07it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.15it/s]\n","                 all         39        111      0.965          1      0.971      0.787\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    71/149     2.07G   0.01921   0.01655 0.0003742        59       416: 100% 26/26 [00:03<00:00,  7.03it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.03it/s]\n","                 all         39        111      0.964          1      0.973      0.789\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    72/149     2.07G   0.01925   0.01604 0.0003883        64       416: 100% 26/26 [00:03<00:00,  7.17it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.04it/s]\n","                 all         39        111      0.966          1      0.969      0.789\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    73/149     2.07G   0.01826   0.01624 0.0003702        60       416: 100% 26/26 [00:03<00:00,  7.04it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.14it/s]\n","                 all         39        111      0.965          1      0.966      0.772\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    74/149     2.07G   0.01911   0.01602 0.0004015        46       416: 100% 26/26 [00:03<00:00,  7.11it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.17it/s]\n","                 all         39        111      0.965          1      0.967      0.781\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    75/149     2.07G   0.01817   0.01516 0.0004001        67       416: 100% 26/26 [00:03<00:00,  7.07it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.08it/s]\n","                 all         39        111      0.966          1      0.967      0.789\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    76/149     2.07G   0.01915   0.01618 0.0003448        60       416: 100% 26/26 [00:03<00:00,  7.15it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.02it/s]\n","                 all         39        111      0.966          1      0.972      0.776\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    77/149     2.07G   0.01843   0.01521 0.0003131        67       416: 100% 26/26 [00:03<00:00,  7.07it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.18it/s]\n","                 all         39        111      0.966          1      0.971      0.774\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    78/149     2.07G   0.02005   0.01669 0.0003548        42       416: 100% 26/26 [00:03<00:00,  7.10it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.11it/s]\n","                 all         39        111      0.966          1      0.972      0.773\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    79/149     2.07G   0.01851   0.01497 0.0003924        75       416: 100% 26/26 [00:03<00:00,  7.02it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.93it/s]\n","                 all         39        111      0.966          1      0.976      0.784\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    80/149     2.07G   0.01814   0.01574 0.0003484        45       416: 100% 26/26 [00:03<00:00,  7.15it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.97it/s]\n","                 all         39        111      0.966          1      0.969      0.787\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    81/149     2.07G   0.01819   0.01609 0.0002836        49       416: 100% 26/26 [00:03<00:00,  7.13it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.01it/s]\n","                 all         39        111      0.966          1      0.971      0.795\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    82/149     2.07G   0.01818   0.01509 0.0003103        48       416: 100% 26/26 [00:03<00:00,  7.08it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.22it/s]\n","                 all         39        111      0.967      0.998       0.97      0.784\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    83/149     2.07G    0.0175   0.01498 0.0002968        31       416: 100% 26/26 [00:03<00:00,  7.04it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.18it/s]\n","                 all         39        111      0.966          1      0.971      0.788\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    84/149     2.07G   0.01788   0.01532 0.0002917        43       416: 100% 26/26 [00:03<00:00,  6.76it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.02it/s]\n","                 all         39        111      0.966          1      0.971      0.775\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    85/149     2.07G   0.01796   0.01446 0.0003022        58       416: 100% 26/26 [00:03<00:00,  7.03it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.04it/s]\n","                 all         39        111      0.966          1      0.971      0.776\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    86/149     2.07G   0.01768   0.01576 0.0003455        67       416: 100% 26/26 [00:03<00:00,  7.14it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.15it/s]\n","                 all         39        111      0.966          1       0.97      0.778\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    87/149     2.07G   0.01776   0.01539 0.0004511        55       416: 100% 26/26 [00:03<00:00,  7.03it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.99it/s]\n","                 all         39        111      0.963      0.999      0.967      0.774\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    88/149     2.07G   0.01768   0.01571 0.0003633        66       416: 100% 26/26 [00:03<00:00,  6.81it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.07it/s]\n","                 all         39        111      0.965          1      0.967      0.775\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    89/149     2.07G    0.0178    0.0148 0.0003781        52       416: 100% 26/26 [00:03<00:00,  7.11it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.93it/s]\n","                 all         39        111      0.964          1      0.973       0.79\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    90/149     2.07G   0.01813   0.01483 0.0002933        41       416: 100% 26/26 [00:03<00:00,  7.12it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.97it/s]\n","                 all         39        111      0.963      0.993      0.972       0.78\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    91/149     2.07G   0.01786   0.01482 0.0002666        57       416: 100% 26/26 [00:03<00:00,  6.90it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.23it/s]\n","                 all         39        111      0.958          1      0.969      0.782\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    92/149     2.07G   0.01741   0.01615  0.000321        57       416: 100% 26/26 [00:03<00:00,  7.05it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.03it/s]\n","                 all         39        111      0.965      0.998      0.965      0.765\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    93/149     2.07G   0.01729   0.01499 0.0004044        38       416: 100% 26/26 [00:03<00:00,  7.11it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.15it/s]\n","                 all         39        111      0.965          1      0.964      0.779\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    94/149     2.07G   0.01726   0.01416 0.0002686        54       416: 100% 26/26 [00:03<00:00,  7.20it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.20it/s]\n","                 all         39        111      0.957          1      0.964      0.785\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    95/149     2.07G   0.01667   0.01535 0.0002492        54       416: 100% 26/26 [00:04<00:00,  5.35it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.20it/s]\n","                 all         39        111      0.957      0.999      0.965      0.783\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    96/149     2.07G   0.01704    0.0148 0.0003869        65       416: 100% 26/26 [00:03<00:00,  7.02it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.04it/s]\n","                 all         39        111      0.965          1      0.967      0.785\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    97/149     2.07G   0.01651   0.01497 0.0002225        55       416: 100% 26/26 [00:03<00:00,  7.11it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.02it/s]\n","                 all         39        111      0.965          1      0.966      0.787\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    98/149     2.07G   0.01644   0.01348 0.0003213        52       416: 100% 26/26 [00:03<00:00,  7.14it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.85it/s]\n","                 all         39        111      0.965          1      0.967      0.787\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","    99/149     2.07G   0.01682   0.01487 0.0002414        61       416: 100% 26/26 [00:03<00:00,  7.09it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.99it/s]\n","                 all         39        111      0.965          1      0.967      0.788\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   100/149     2.07G   0.01711   0.01492 0.0003601        61       416: 100% 26/26 [00:03<00:00,  7.10it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.94it/s]\n","                 all         39        111      0.963          1       0.97      0.783\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   101/149     2.07G   0.01701   0.01474  0.000248        54       416: 100% 26/26 [00:03<00:00,  6.97it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.07it/s]\n","                 all         39        111      0.962          1      0.967      0.778\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   102/149     2.07G   0.01636   0.01517 0.0002174        41       416: 100% 26/26 [00:03<00:00,  7.09it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.27it/s]\n","                 all         39        111      0.966          1      0.969      0.775\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   103/149     2.07G   0.01678   0.01442 0.0002257        38       416: 100% 26/26 [00:03<00:00,  6.99it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.79it/s]\n","                 all         39        111      0.965          1      0.968      0.775\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   104/149     2.07G   0.01625   0.01443 0.0002145        66       416: 100% 26/26 [00:03<00:00,  7.15it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.70it/s]\n","                 all         39        111      0.964          1      0.968      0.767\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   105/149     2.07G   0.01603   0.01326 0.0002057        46       416: 100% 26/26 [00:03<00:00,  6.80it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.11it/s]\n","                 all         39        111      0.965          1      0.968      0.763\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   106/149     2.07G   0.01647   0.01519 0.0002138        52       416: 100% 26/26 [00:03<00:00,  7.04it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.08it/s]\n","                 all         39        111      0.966      0.999       0.97       0.77\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   107/149     2.07G   0.01647   0.01509 0.0002005        57       416: 100% 26/26 [00:03<00:00,  6.99it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.88it/s]\n","                 all         39        111      0.959          1      0.969      0.777\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   108/149     2.07G   0.01634    0.0144 0.0002166        49       416: 100% 26/26 [00:03<00:00,  7.10it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.09it/s]\n","                 all         39        111      0.961          1      0.969      0.768\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   109/149     2.07G   0.01611   0.01532 0.0001656        68       416: 100% 26/26 [00:03<00:00,  6.90it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.11it/s]\n","                 all         39        111       0.96          1       0.97      0.771\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   110/149     2.07G   0.01603    0.0142 0.0001682        53       416: 100% 26/26 [00:03<00:00,  7.03it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.11it/s]\n","                 all         39        111       0.96          1      0.972      0.773\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   111/149     2.07G   0.01581    0.0135 0.0001624        56       416: 100% 26/26 [00:03<00:00,  7.12it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.10it/s]\n","                 all         39        111      0.965          1       0.97      0.773\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   112/149     2.07G   0.01604   0.01342 0.0001554        44       416: 100% 26/26 [00:03<00:00,  7.10it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.92it/s]\n","                 all         39        111      0.966          1       0.97      0.773\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   113/149     2.07G   0.01592   0.01437 0.0001791        42       416: 100% 26/26 [00:03<00:00,  6.99it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.04it/s]\n","                 all         39        111      0.964          1      0.971      0.785\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   114/149     2.07G   0.01558   0.01462  0.000226        68       416: 100% 26/26 [00:03<00:00,  6.99it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.97it/s]\n","                 all         39        111      0.964          1      0.972      0.784\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   115/149     2.07G    0.0155   0.01356 0.0002088        57       416: 100% 26/26 [00:03<00:00,  6.91it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.11it/s]\n","                 all         39        111      0.962          1      0.973      0.783\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   116/149     2.07G   0.01552   0.01469 0.0001633        88       416: 100% 26/26 [00:03<00:00,  6.99it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.95it/s]\n","                 all         39        111       0.96          1      0.973      0.782\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   117/149     2.07G   0.01536   0.01362 0.0001337        67       416: 100% 26/26 [00:03<00:00,  6.96it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.22it/s]\n","                 all         39        111      0.959          1      0.971      0.778\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   118/149     2.07G   0.01566   0.01447 0.0001452        57       416: 100% 26/26 [00:03<00:00,  7.08it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.00it/s]\n","                 all         39        111       0.96          1      0.972      0.777\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   119/149     2.07G   0.01583   0.01445  0.000161        59       416: 100% 26/26 [00:03<00:00,  6.96it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.26it/s]\n","                 all         39        111       0.96          1      0.973      0.786\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   120/149     2.07G   0.01579   0.01375 0.0001392        53       416: 100% 26/26 [00:03<00:00,  7.07it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.89it/s]\n","                 all         39        111      0.958          1      0.974      0.781\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   121/149     2.07G   0.01488    0.0144 0.0001504        75       416: 100% 26/26 [00:03<00:00,  6.89it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.30it/s]\n","                 all         39        111       0.96          1      0.971      0.771\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   122/149     2.07G   0.01512   0.01347 0.0001213        64       416: 100% 26/26 [00:03<00:00,  7.14it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.16it/s]\n","                 all         39        111       0.96          1      0.972       0.78\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   123/149     2.07G    0.0152   0.01494 0.0001864        88       416: 100% 26/26 [00:03<00:00,  6.96it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.04it/s]\n","                 all         39        111      0.962          1      0.972      0.779\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   124/149     2.07G    0.0152   0.01372 0.0001259        50       416: 100% 26/26 [00:04<00:00,  6.15it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  4.91it/s]\n","                 all         39        111      0.965          1      0.972      0.785\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   125/149     2.07G   0.01492   0.01338 0.0002134        44       416: 100% 26/26 [00:04<00:00,  6.34it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.29it/s]\n","                 all         39        111      0.962          1      0.972      0.779\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   126/149     2.07G   0.01519   0.01341 0.0001237        47       416: 100% 26/26 [00:03<00:00,  7.02it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.03it/s]\n","                 all         39        111      0.965          1      0.973       0.78\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   127/149     2.07G   0.01473   0.01407 0.0002448        46       416: 100% 26/26 [00:03<00:00,  6.94it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.02it/s]\n","                 all         39        111      0.966          1      0.973       0.78\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   128/149     2.07G   0.01486   0.01321 0.0001146        46       416: 100% 26/26 [00:03<00:00,  7.01it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.00it/s]\n","                 all         39        111      0.966          1      0.974      0.771\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   129/149     2.07G   0.01493   0.01423 0.0001273        40       416: 100% 26/26 [00:03<00:00,  6.99it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.99it/s]\n","                 all         39        111      0.965          1      0.973      0.773\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   130/149     2.07G   0.01459   0.01365 0.0002182        75       416: 100% 26/26 [00:03<00:00,  7.12it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.32it/s]\n","                 all         39        111      0.965          1      0.973      0.778\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   131/149     2.07G   0.01496    0.0135 0.0001143        55       416: 100% 26/26 [00:03<00:00,  6.92it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.06it/s]\n","                 all         39        111      0.966          1      0.973       0.78\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   132/149     2.07G   0.01478   0.01358 0.0001271        51       416: 100% 26/26 [00:03<00:00,  7.05it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.80it/s]\n","                 all         39        111      0.965          1      0.973      0.776\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   133/149     2.07G    0.0149   0.01423 9.446e-05        44       416: 100% 26/26 [00:03<00:00,  6.97it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.15it/s]\n","                 all         39        111      0.965          1      0.973      0.778\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   134/149     2.07G   0.01447   0.01425 9.369e-05        48       416: 100% 26/26 [00:03<00:00,  7.10it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.01it/s]\n","                 all         39        111      0.965          1      0.973      0.767\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   135/149     2.07G   0.01457   0.01332 0.0001986        58       416: 100% 26/26 [00:03<00:00,  7.12it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.11it/s]\n","                 all         39        111      0.965          1      0.973       0.77\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   136/149     2.07G   0.01435   0.01359 8.883e-05        61       416: 100% 26/26 [00:03<00:00,  7.06it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.58it/s]\n","                 all         39        111      0.965          1      0.973      0.783\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   137/149     2.07G   0.01441   0.01343  0.000119        41       416: 100% 26/26 [00:03<00:00,  7.03it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.01it/s]\n","                 all         39        111      0.964          1      0.974       0.78\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   138/149     2.07G   0.01441   0.01316 0.0001093        54       416: 100% 26/26 [00:03<00:00,  7.02it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.06it/s]\n","                 all         39        111      0.964          1      0.975      0.774\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   139/149     2.07G   0.01466   0.01318 9.212e-05        43       416: 100% 26/26 [00:03<00:00,  6.94it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.26it/s]\n","                 all         39        111      0.964          1      0.975      0.777\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   140/149     2.07G   0.01457    0.0136 0.0001204        65       416: 100% 26/26 [00:03<00:00,  7.16it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.84it/s]\n","                 all         39        111      0.964          1      0.975      0.783\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   141/149     2.07G   0.01492   0.01285  9.04e-05        62       416: 100% 26/26 [00:03<00:00,  6.99it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.93it/s]\n","                 all         39        111      0.965          1      0.975      0.778\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   142/149     2.07G   0.01455   0.01352 8.576e-05        70       416: 100% 26/26 [00:03<00:00,  6.99it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.01it/s]\n","                 all         39        111      0.965          1      0.975      0.781\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   143/149     2.07G   0.01435   0.01312 7.935e-05        42       416: 100% 26/26 [00:03<00:00,  6.85it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.15it/s]\n","                 all         39        111      0.966          1      0.974      0.773\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   144/149     2.07G   0.01441   0.01291 0.0001193        49       416: 100% 26/26 [00:03<00:00,  7.00it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.32it/s]\n","                 all         39        111      0.966          1      0.975      0.773\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   145/149     2.07G   0.01402   0.01303 8.117e-05        73       416: 100% 26/26 [00:03<00:00,  6.98it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.01it/s]\n","                 all         39        111      0.965          1      0.974      0.773\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   146/149     2.07G   0.01439   0.01366 9.852e-05        64       416: 100% 26/26 [00:03<00:00,  6.73it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.98it/s]\n","                 all         39        111      0.965          1      0.974      0.775\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   147/149     2.07G   0.01435   0.01314 9.481e-05        47       416: 100% 26/26 [00:03<00:00,  6.99it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.10it/s]\n","                 all         39        111      0.965          1      0.974      0.772\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   148/149     2.07G   0.01433   0.01388 0.0001287        67       416: 100% 26/26 [00:03<00:00,  7.06it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  7.18it/s]\n","                 all         39        111      0.965          1      0.974       0.77\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","   149/149     2.07G   0.01437   0.01339 9.596e-05        56       416: 100% 26/26 [00:03<00:00,  6.94it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  6.83it/s]\n","                 all         39        111      0.965          1      0.974      0.772\n","\n","150 epochs completed in 0.185 hours.\n","Optimizer stripped from runs/train/exp/weights/last.pt, 14.3MB\n","Optimizer stripped from runs/train/exp/weights/best.pt, 14.3MB\n","\n","Validating runs/train/exp/weights/best.pt...\n","Fusing layers... \n","Model summary: 213 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  3.30it/s]\n","                 all         39        111      0.966          1      0.971      0.795\n","       Ahmad Firdaus         39         39      0.997          1      0.995      0.868\n","              Others         39         72      0.934          1      0.947      0.722\n","Results saved to \u001b[1mruns/train/exp\u001b[0m\n"]}]},{"cell_type":"code","source":["!python detect.py --weights runs/train/exp/weights/best.pt --img 416 --conf 0.1 --source {dataset.location}/test/images"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3blUH9YSmD9e","executionInfo":{"status":"ok","timestamp":1659344667270,"user_tz":-480,"elapsed":13851,"user":{"displayName":"Hajar 010","userId":"00191370975495187311"}},"outputId":"7dd388e2-8abc-4040-9c2e-ea464510277f"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/best.pt'], source=/content/datasets/Person_Recognition-1/test/images, data=data/coco128.yaml, imgsz=[416, 416], conf_thres=0.1, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n","YOLOv5  v6.1-347-g7b9cc32 Python-3.7.13 torch-1.12.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n","\n","Fusing layers... \n","Model summary: 213 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n","image 1/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_24_54_Pro_jpg.rf.00f1fef1300a406b60beacc8da2cbbaa.jpg: 416x416 1 Ahmad Firdaus, 4 Otherss, Done. (0.018s)\n","image 2/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_24_55_Pro--4-_jpg.rf.f852a9073e68f102ccb7e23e4d007efe.jpg: 416x416 1 Ahmad Firdaus, 4 Otherss, Done. (0.018s)\n","image 3/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_24_55_Pro--6-_jpg.rf.5272786faf026807180b07afcdc1ebcb.jpg: 416x416 1 Ahmad Firdaus, 4 Otherss, Done. (0.027s)\n","image 4/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_24_59_Pro--5-_jpg.rf.444af4a5dc2118e91c95cc721e82e890.jpg: 416x416 1 Ahmad Firdaus, 4 Otherss, Done. (0.021s)\n","image 5/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_25_00_Pro--1-_jpg.rf.a703cbcf95ce64013eb96b89735bd1ef.jpg: 416x416 1 Ahmad Firdaus, 4 Otherss, Done. (0.013s)\n","image 6/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_25_02_Pro--1-_jpg.rf.69684b44fa46cceacc6fa97ecf6b940f.jpg: 416x416 1 Ahmad Firdaus, 4 Otherss, Done. (0.014s)\n","image 7/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_25_02_Pro--5-_jpg.rf.f945e5184f665a3a540f347ea23b9d45.jpg: 416x416 1 Ahmad Firdaus, 4 Otherss, Done. (0.050s)\n","image 8/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_25_05_Pro--6-_jpg.rf.c10b125de0a9de323ee1037a2e2a8cca.jpg: 416x416 1 Ahmad Firdaus, 5 Otherss, Done. (0.013s)\n","image 9/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_25_06_Pro_jpg.rf.bb35b8836cfcdf4bf066f26c5075689c.jpg: 416x416 1 Ahmad Firdaus, 4 Otherss, Done. (0.013s)\n","image 10/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_25_07_Pro--2-_jpg.rf.ab8794aef2174086806713118cd68366.jpg: 416x416 1 Ahmad Firdaus, 7 Otherss, Done. (0.013s)\n","image 11/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_28_25_Pro_jpg.rf.add6ee7fff1ea2581fde088419c73804.jpg: 416x416 1 Ahmad Firdaus, Done. (0.020s)\n","image 12/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_28_39_Pro_jpg.rf.8605f5bee0c1ad5be9bc0329cbed13c0.jpg: 416x416 1 Ahmad Firdaus, Done. (0.013s)\n","image 13/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220725_12_28_45_Pro_jpg.rf.bee76a2bd01548471dc39ee37469e673.jpg: 416x416 1 Ahmad Firdaus, Done. (0.023s)\n","image 14/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220801_15_14_19_Pro--3-_jpg.rf.b79b16774d3ac2026870dfb3fc31729b.jpg: 416x416 1 Ahmad Firdaus, Done. (0.012s)\n","image 15/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220801_15_14_22_Pro--2-_jpg.rf.2954932a3c4c447df3599e92e0391e1c.jpg: 416x416 1 Ahmad Firdaus, Done. (0.014s)\n","image 16/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220801_15_14_24_Pro--3-_jpg.rf.003c117bb9b9974f606686079775b4b3.jpg: 416x416 1 Ahmad Firdaus, Done. (0.019s)\n","image 17/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220801_15_14_24_Pro--5-_jpg.rf.222c7f2b2f10d0f414311157eeb1eccb.jpg: 416x416 1 Ahmad Firdaus, Done. (0.012s)\n","image 18/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220801_15_14_38_Pro_jpg.rf.052122f3f647e0a581c2a018682fdab2.jpg: 416x416 1 Ahmad Firdaus, 1 Others, Done. (0.012s)\n","image 19/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220801_15_14_39_Pro--3-_jpg.rf.9b18934ab084a4b856514dd7f7aabc5d.jpg: 416x416 1 Ahmad Firdaus, 1 Others, Done. (0.012s)\n","image 20/20 /content/datasets/Person_Recognition-1/test/images/WIN_20220801_15_14_41_Pro--5-_jpg.rf.f857335207e3c220d2f9dccdc8666833.jpg: 416x416 1 Ahmad Firdaus, 1 Others, Done. (0.013s)\n","Speed: 0.7ms pre-process, 17.5ms inference, 1.8ms NMS per image at shape (1, 3, 416, 416)\n","Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"]}]},{"cell_type":"code","source":["#export your model's weights for future use\n","from google.colab import files\n","files.download('./runs/train/exp/weights/best.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"0PragXuzfbWF","executionInfo":{"status":"ok","timestamp":1659344667271,"user_tz":-480,"elapsed":15,"user":{"displayName":"Hajar 010","userId":"00191370975495187311"}},"outputId":"58f1995e-4ed0-432d-ba49-7d2f0aea0fcd"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_9728833b-d7bc-4300-a74e-76bd952df0c7\", \"best.pt\", 14339381)"]},"metadata":{}}]},{"cell_type":"code","source":["# Run on Conda Prompt #\n","#python detect.py --weights best.pt --source 0"],"metadata":{"id":"5EFvaSr9iimo"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"Assessment1_1814113.ipynb","provenance":[{"file_id":"https://gist.github.com/CUTe-EmbeddedAI/2087a1af9536bc516a39b6b0d18d2a23#file-assessment1_deeplearning-ipynb","timestamp":1659316788073}],"collapsed_sections":[]},"gpuClass":"standard","accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"408ab0dbe4e74807b894ff5cb9d2573b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da542b18762b46719fa29e0fcc99d8b2","IPY_MODEL_8d080be0b97e42ebae951db30d73c565","IPY_MODEL_f205e8afb32e485f864c474bce5ab8e5"],"layout":"IPY_MODEL_0a5445134fcd4f91b09fdd91f2c20d5b"}},"da542b18762b46719fa29e0fcc99d8b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6309bed8edf742fca7594e8aa47d2968","placeholder":"","style":"IPY_MODEL_ca46a4dba9fb45d48fd9d541ecd05bee","value":"100%"}},"8d080be0b97e42ebae951db30d73c565":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd79f8a347884c8d9067bb6c604bba63","max":241627721,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7616ab1dca3449418511d552c539937a","value":241627721}},"f205e8afb32e485f864c474bce5ab8e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a0a3fd19df9465eab9cb2af76a0cc4d","placeholder":"","style":"IPY_MODEL_54131bd4f6cc46198b4133d0b8030427","value":" 230M/230M [00:01&lt;00:00, 171MB/s]"}},"0a5445134fcd4f91b09fdd91f2c20d5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6309bed8edf742fca7594e8aa47d2968":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca46a4dba9fb45d48fd9d541ecd05bee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd79f8a347884c8d9067bb6c604bba63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7616ab1dca3449418511d552c539937a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a0a3fd19df9465eab9cb2af76a0cc4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54131bd4f6cc46198b4133d0b8030427":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}